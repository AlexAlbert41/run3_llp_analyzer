{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06a549e7",
   "metadata": {},
   "source": [
    "## Data/MC Comparison for Brem-Induced Clusters\n",
    "\n",
    "#### Comparison of Clusters in Data vs MC. This is to validate the signal reconstruction process. We compare cluster from Z->MuMu events in Data vs those from a DY->ZMuMu (50-120 GeV for MLL). Data is from 2023B&C, and MC is from the preBPix, normalized to the appropriate value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea6d707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TInterpreter::ReadRootmapFile>: class  HepMC::FourVector found in libSimDataFormatsGeneratorProducts.so  is already in libHepMC3rootIO.so \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.28/00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/sft.cern.ch/lcg/views/LCG_103/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/cvmfs/sft.cern.ch/lcg/views/LCG_103/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/cvmfs/sft.cern.ch/lcg/views/LCG_103/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/cvmfs/sft.cern.ch/lcg/views/LCG_103/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uproot\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0,\"../\")\n",
    "import mplhep as hep\n",
    "import pickle\n",
    "import glob\n",
    "import ROOT as rt\n",
    "import coffea\n",
    "import awkward as ak\n",
    "from coffea import hist, processor\n",
    "from coffea.nanoevents.methods import candidate\n",
    "from coffea.nanoevents.methods import vector\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e77ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import helper modules for muon scale factor computation\n",
    "sys.path.append(\"/uscms/home/amalbert/nobackup/CMSSW_14_1_0_pre4/src/RazorCommon/Tools/bin\")\n",
    "import importlib\n",
    "import getMuonScaleFactor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c873b2b3",
   "metadata": {},
   "source": [
    "#### Load ntuples as awkward arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afd254d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.behavior.update(candidate.behavior)\n",
    "\n",
    "def getLZDF(f,nEvents=-1,version=\"new\"): #lazy dataframe with events that have cluster matched to probe muon\n",
    "    events_raw = uproot.open(f)['MuonSystem']\n",
    "    df = coffea.processor.LazyDataFrame(events_raw,entrystop=nEvents)\n",
    "    start,stop = df._branchargs['entry_start'],df._branchargs['entry_stop']\n",
    "    events = uproot.lazy(df._tree)\n",
    "    #events = events[start:stop]\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98a02b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "MC_paths = {#\"2022\":\"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/MC_noiseFilters/MC_Summer22/DYto2Mu_MLL-50to120_keepMDSHits_Merged/DYto2Mu_MLL-50to120_keepMDSHits_7980pb_weighted.root\",\n",
    "            \"2022EE\":\"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/MC_noiseFilters_fixed/MC_Summer22EE/DYto2Mu_MLL-50to120_keepMDSHits_Merged/DYto2Mu_MLL-50to120_keepMDSHits_26642pb_weighted.root\",\n",
    "            \"2023\":\"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/MC_noiseFilters_fixed/MC_Summer23/DYto2Mu_MLL-50to120_Merged/DYto2Mu_MLL-50to120_18411pb_weighted.root\",\n",
    "            \"2023BPix\":\"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/MC_noiseFilters_fixed/MC_Summer23BPix/DYto2Mu_MLL-50to120_Merged/DYto2Mu_MLL-50to120_9451pb_weighted.root\"}\n",
    "\n",
    "data_path_lists = {#\"2022\":[\"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/Data_noiseFilters_fixed/2022_Merged/Muon_Run2022C_PromptReco-v1_goodLumi.root\",\n",
    "                   #       \"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/Data_noiseFilters_fixed/2022_Merged/Muon_Run2022D_PromptReco-v1_goodLumi.root\"],\n",
    "                 \n",
    "                   \"2022EE\":[\"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/Data_noiseFilters_fixed/2022_Merged/Muon_Run2022E_PromptReco-v1_goodLumi.root\",\n",
    "                          \"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/Data_noiseFilters_fixed/2022_Merged/Muon_Run2022F_PromptReco-v1_goodLumi.root\",\n",
    "                            \"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/Data_noiseFilters_fixed/2022_Merged/Muon_Run2022G_PromptReco-v1_goodLumi.root\"],\n",
    "                   \n",
    "                    \"2023\":[\"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/Data_noiseFilters_fixed/2023_Merged/Muon0_Run2023B_PromptReco-v1_goodLumi.root\",\n",
    "                     \"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/Data_noiseFilters_fixed/2023_Merged/Muon1_Run2023B_PromptReco-v1_goodLumi.root\",\n",
    "                     \"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/Data_noiseFilters_fixed/2023_Merged/Muon0_Run2023C_PromptReco-v1_goodLumi.root\",\n",
    "                     \"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/Data_noiseFilters_fixed/2023_Merged/Muon1_Run2023C_PromptReco-v1_goodLumi.root\",\n",
    "                     \"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/Data_noiseFilters_fixed/2023_Merged/Muon0_Run2023C_PromptReco-v2_goodLumi.root\",\n",
    "                     \"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/Data_noiseFilters_fixed/2023_Merged/Muon1_Run2023C_PromptReco-v2_goodLumi.root\",\n",
    "                     \"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/Data_noiseFilters_fixed/2023_Merged/Muon0_Run2023C_PromptReco-v3_goodLumi.root\",\n",
    "                     \"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/Data_noiseFilters_fixed/2023_Merged/Muon1_Run2023C_PromptReco-v3_goodLumi.root\",\n",
    "                     \"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/Data_noiseFilters_fixed/2023_Merged/Muon0_Run2023C_PromptReco-v4_goodLumi.root\",\n",
    "                     \"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/Data_noiseFilters_fixed/2023_Merged/Muon1_Run2023C_PromptReco-v4_goodLumi.root\"],\n",
    "                  \n",
    "                      \"2023BPix\":[\"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/Data_noiseFilters_fixed/2023_Merged/Muon0_Run2023D_PromptReco-v1_goodLumi.root\",\n",
    "                     \"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/Data_noiseFilters_fixed/2023_Merged/Muon1_Run2023D_PromptReco-v1_goodLumi.root\",\n",
    "                     \"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/Data_noiseFilters_fixed/2023_Merged/Muon0_Run2023D_PromptReco-v2_goodLumi.root\",\n",
    "                     \"/store/group/lpclonglived/amalbert/Data_MC_Comp_TnP/results_from_cache_noSkim/Data_noiseFilters_fixed/2023_Merged/Muon1_Run2023D_PromptReco-v2_goodLumi.root\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7c3d287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022EE\n",
      "2023\n",
      "2023BPix\n",
      "2022EE\n",
      "2023\n",
      "2023BPix\n"
     ]
    }
   ],
   "source": [
    "events_MC_full_dict= {}\n",
    "for campaign, MC_path in MC_paths.items():\n",
    "    print(campaign)\n",
    "    if campaign!=\"2022EE\":continue\n",
    "    events_MC_full = getLZDF(\"root://cmseos.fnal.gov/\"+MC_path)\n",
    "    events_MC_full = events_MC_full[events_MC_full.nCscRechitClusters>0]\n",
    "    events_MC_full = events_MC_full[np.logical_and(events_MC_full.ZMass>50, events_MC_full.ZMass<120)]\n",
    "    events_MC_full = events_MC_full[events_MC_full.Flag_all]\n",
    "    events_MC_full = events_MC_full[events_MC_full.Flag_ecalBadCalibFilter]\n",
    "    events_MC_full = events_MC_full[events_MC_full.jetVeto]\n",
    "    events_MC_full_dict[campaign] = events_MC_full\n",
    "    \n",
    "events_data_full_dict= {}\n",
    "for campaign, data_path_list in data_path_lists.items():\n",
    "    print(campaign)\n",
    "    if campaign!=\"2022EE\":continue\n",
    "    data_events = [getLZDF(\"root://cmseos.fnal.gov/\"+data_path) for data_path in data_path_list]\n",
    "    events_data_full = ak.concatenate(data_events, axis=0)\n",
    "    events_data_full = events_data_full[events_data_full.nCscRechitClusters>0]\n",
    "    events_data_full = events_data_full[np.logical_and(events_data_full.ZMass>50, events_data_full.ZMass<120)]\n",
    "    events_data_full = events_data_full[events_data_full.Flag_all]\n",
    "    events_data_full = events_data_full[events_data_full.Flag_ecalBadCalibFilter]\n",
    "    events_data_full = events_data_full[events_data_full.jetVeto]\n",
    "    events_data_full_dict[campaign] = events_data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46179a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_kFactors = {\"2022\":1,\"2022EE\":0.8425135156354998,\"2023\":0.9287192347533128,\"2023BPix\":0.9470132857601179}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1940b07",
   "metadata": {},
   "source": [
    "### modify the input ntuples so that each entry corresponds with a cluster. As a result, some entries will be repeated twice (tne ones denoted \"branch names\") if there are two clusters in the event. At this step, all of the branches that we compute for the measurement should be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "316d033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define cluster level csc branches needed\n",
    "csc_branches = []; csc_chamber_hit_branches = []\n",
    "for branch_name in events_MC_full.fields: \n",
    "    if \"csc\" in branch_name and \"dt\" not in branch_name and \"LLP\" not in branch_name and \"DNN\" not in branch_name:\n",
    "        csc_branches.append(branch_name)\n",
    "    if \"cscRechitClusterNRechit\" in branch_name:\n",
    "        csc_chamber_hit_branches.append(branch_name)\n",
    "csc_chamber_hit_branches = np.array(csc_chamber_hit_branches)\n",
    "ME11MinusIndex = np.where(csc_chamber_hit_branches==\"cscRechitClusterNRechitChamberMinus11\")[0]\n",
    "ME12MinusIndex = np.where(csc_chamber_hit_branches==\"cscRechitClusterNRechitChamberMinus12\")[0]\n",
    "ME11PlusIndex = np.where(csc_chamber_hit_branches==\"cscRechitClusterNRechitChamberPlus11\")[0]\n",
    "ME12PlusIndex = np.where(csc_chamber_hit_branches==\"cscRechitClusterNRechitChamberPlus12\")[0]\n",
    "\n",
    "forward_chamber_field_indices = [ME11MinusIndex, ME12MinusIndex, ME11PlusIndex, ME12PlusIndex]\n",
    "#event-level branches        \n",
    "branch_names = [\"runNum\", \"evtNum\", \"weight\", \"pileupWeight\", \"ZMass\", \"met\", \"metPhi\", \"puppiMet\", \"puppiMetPhi\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6ba9761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make more useful input awkward array, with all information in cluster-level format\n",
    "def getClusterBranches(LZDF, campaign, isMC=False):\n",
    "    new_df = ak.zip({field: ak.flatten(LZDF[field]) for field in csc_branches})\n",
    "    \n",
    "    #compute cluster max chamber\n",
    "    hits_by_chamber = np.stack([ak.flatten(LZDF[branch]) for branch in csc_chamber_hit_branches], axis=1)\n",
    "    #print(hits_by_chamber)\n",
    "    #print(np.array(hits_by_chamber).shape)\n",
    "    maxBranchIndex = np.argmax(hits_by_chamber, axis=1)\n",
    "    #print(maxBranchIndex)\n",
    "    chamber_masks_lists = []\n",
    "    for chamber_index in forward_chamber_field_indices:\n",
    "        chamber_masks_lists.append((maxBranchIndex==chamber_index))\n",
    "        #print(np.where(maxBranchIndex==chamber_index))\n",
    "    #print(np.stack(chamber_masks_lists, axis=1))\n",
    "    new_df = ak.with_field(new_df, np.any(np.stack(chamber_masks_lists, axis=1), axis=1), \"forward_max_chamber\")\n",
    "    \n",
    "    print(\"finished csc branches\")\n",
    "    newDNN = ak.flatten(ak.mask(LZDF[\"cscRechitClusterDNN_bkgMC_plusBeamHalo\"], LZDF[\"cscRechitClusterDNN_bkgMC_plusBeamHalo\"]>0))\n",
    "    newDNN = newDNN[~ak.is_none(newDNN)]\n",
    "    new_df = ak.with_field(new_df, newDNN, \"cscRechitClusterDNN_bkgMC_plusBeamHalo\")\n",
    "    \n",
    "    for branch in branch_names:\n",
    "        if (not isMC) and (branch in [\"weight\", \"pileupWeight\"]):\n",
    "            continue\n",
    "        new_df = ak.with_field(new_df, np.repeat(LZDF[branch],LZDF[\"nCscRechitClusters\"]), branch)\n",
    "\n",
    "    \n",
    "    column_indices_probe = np.array(ak.flatten(ak.values_astype(LZDF[\"cscRechitCluster_matchToMuon2\"], int)))\n",
    "    column_indices_tag = np.array(ak.flatten(ak.values_astype(LZDF[\"cscRechitCluster_matchToMuon1\"], int)))\n",
    "    row_indices = np.arange(np.size(column_indices_probe), dtype=int)\n",
    "    \n",
    "    #compute \n",
    "    if isMC:\n",
    "        MC_SF_LooseID = getMuonScaleFactor.getLooseIDEffArr_preBPix(np.array(LZDF.lepPt), np.array(LZDF.lepEta))\n",
    "        MC_SF_LooseISO = getMuonScaleFactor.getLooseISOEffArr_preBPix(np.array(LZDF.lepPt), np.array(LZDF.lepEta))\n",
    "        MC_SF_TightID = getMuonScaleFactor.getTightIDEffArr_preBPix(np.array(LZDF.lepPt), np.array(LZDF.lepEta))\n",
    "        MC_SF_TightISO = getMuonScaleFactor.getTightISOEffArr_preBPix(np.array(LZDF.lepPt), np.array(LZDF.lepEta))\n",
    "        MC_SF_HLT = getMuonScaleFactor.getHLTEffArr_preBPix(np.array(LZDF.lepPt), np.array(LZDF.lepEta))\n",
    "        \n",
    "        MC_SF_LooseID = np.repeat(MC_SF_LooseID,np.array(LZDF[\"nCscRechitClusters\"]), axis=0)[row_indices,column_indices_probe]\n",
    "        MC_SF_LooseISO = np.repeat(np.array(MC_SF_LooseISO),np.array(LZDF[\"nCscRechitClusters\"]), axis=0)[row_indices,column_indices_probe]\n",
    "        MC_SF_TightID = np.repeat(MC_SF_TightID,np.array(LZDF[\"nCscRechitClusters\"]), axis=0)[row_indices,column_indices_tag]\n",
    "        MC_SF_TightISO = np.repeat(MC_SF_TightISO,np.array(LZDF[\"nCscRechitClusters\"]), axis=0)[row_indices,column_indices_tag]\n",
    "        MC_SF_HLT = np.repeat(MC_SF_HLT,np.array(LZDF[\"nCscRechitClusters\"]), axis=0)[row_indices,column_indices_tag]\n",
    "        \n",
    "        MC_Weight_Total = new_df[\"weight\"]*new_df[\"pileupWeight\"]*MC_SF_LooseID*MC_SF_LooseISO*MC_SF_TightID*MC_SF_TightISO*MC_SF_HLT*MC_kFactors[campaign]\n",
    "        new_df = ak.with_field(new_df, MC_Weight_Total, \"weight_total\")\n",
    "    \n",
    "    print(\"at muon variables\")\n",
    "    \n",
    "#     #load pT, eta, and phi for tag and probe muons\n",
    "#     probe_pT = np.repeat(np.array(LZDF[\"lepPt\"]),np.array(LZDF[\"nCscRechitClusters\"]), axis=0)[row_indices,column_indices_probe]\n",
    "#     probe_eta = np.repeat(np.array(LZDF[\"lepEta\"]),np.array(LZDF[\"nCscRechitClusters\"]), axis=0)[row_indices,column_indices_probe]\n",
    "#     probe_phi = np.repeat(np.array(LZDF[\"lepPhi\"]),np.array(LZDF[\"nCscRechitClusters\"]), axis=0)[row_indices,column_indices_probe]\n",
    "    \n",
    "#     tag_pT = np.repeat(np.array(LZDF[\"lepPt\"]),np.array(LZDF[\"nCscRechitClusters\"]), axis=0)[row_indices,column_indices_tag]\n",
    "#     tag_eta = np.repeat(np.array(LZDF[\"lepEta\"]),np.array(LZDF[\"nCscRechitClusters\"]), axis=0)[row_indices,column_indices_tag]\n",
    "#     tag_phi = np.repeat(np.array(LZDF[\"lepPhi\"]),np.array(LZDF[\"nCscRechitClusters\"]), axis=0)[row_indices,column_indices_tag]\n",
    "    \n",
    "#     new_df = ak.with_field(new_df, probe_pT, \"probe_pT\")\n",
    "#     new_df = ak.with_field(new_df, probe_eta, \"probe_eta\")\n",
    "#     new_df = ak.with_field(new_df, probe_phi, \"probe_phi\")\n",
    "    \n",
    "#     new_df = ak.with_field(new_df, tag_pT, \"tag_pT\")\n",
    "#     new_df = ak.with_field(new_df, tag_eta, \"tag_eta\")\n",
    "#     new_df = ak.with_field(new_df, tag_phi, \"tag_phi\")\n",
    "    \n",
    "#     #deltaR(cluster, muon)\n",
    "#     new_df = ak.with_field(new_df, np.sqrt((new_df[\"cscRechitClusterEta\"]-new_df[\"probe_eta\"])**2+(new_df[\"cscRechitClusterPhi\"]-new_df[\"probe_phi\"])**2), \"cscRechitClusterMuonDeltaR\")\n",
    "    \n",
    "#     #DNN inputs - hit fractions in stations/rings\n",
    "#     new_df = ak.with_field(new_df, (new_df.cscRechitClusterNRechitChamberPlus11+new_df.cscRechitClusterNRechitChamberMinus11+new_df.cscRechitClusterNRechitChamberPlus12+new_df.cscRechitClusterNRechitChamberMinus12+new_df.cscRechitClusterNRechitChamberPlus13+new_df.cscRechitClusterNRechitChamberMinus13)/new_df.cscRechitClusterSize, \"cscRechitClusterFracS1\")\n",
    "#     new_df = ak.with_field(new_df, (new_df.cscRechitClusterNRechitChamberPlus21+new_df.cscRechitClusterNRechitChamberMinus21+new_df.cscRechitClusterNRechitChamberPlus22+new_df.cscRechitClusterNRechitChamberMinus22)/new_df.cscRechitClusterSize, \"cscRechitClusterFracS2\")\n",
    "#     new_df = ak.with_field(new_df, (new_df.cscRechitClusterNRechitChamberPlus31+new_df.cscRechitClusterNRechitChamberMinus31+new_df.cscRechitClusterNRechitChamberPlus32+new_df.cscRechitClusterNRechitChamberMinus32)/new_df.cscRechitClusterSize, \"cscRechitClusterFracS3\")\n",
    "#     new_df = ak.with_field(new_df, (new_df.cscRechitClusterNRechitChamberPlus41+new_df.cscRechitClusterNRechitChamberMinus41+new_df.cscRechitClusterNRechitChamberPlus42+new_df.cscRechitClusterNRechitChamberMinus42)/new_df.cscRechitClusterSize, \"cscRechitClusterFracS4\")\n",
    "\n",
    "#     new_df = ak.with_field(new_df,(new_df.cscRechitClusterNRechitChamberPlus11+new_df.cscRechitClusterNRechitChamberMinus11+new_df.cscRechitClusterNRechitChamberPlus21+new_df.cscRechitClusterNRechitChamberMinus21+new_df.cscRechitClusterNRechitChamberPlus31+new_df.cscRechitClusterNRechitChamberMinus31+new_df.cscRechitClusterNRechitChamberPlus41+new_df.cscRechitClusterNRechitChamberMinus41)/new_df.cscRechitClusterSize, \"cscRechitClusterFracR1\")\n",
    "#     new_df = ak.with_field(new_df, (new_df.cscRechitClusterNRechitChamberPlus12+new_df.cscRechitClusterNRechitChamberMinus12+new_df.cscRechitClusterNRechitChamberPlus22+new_df.cscRechitClusterNRechitChamberMinus22+new_df.cscRechitClusterNRechitChamberPlus32+new_df.cscRechitClusterNRechitChamberMinus32+new_df.cscRechitClusterNRechitChamberPlus42+new_df.cscRechitClusterNRechitChamberMinus42)/new_df.cscRechitClusterSize, \"cscRechitClusterFracR2\")\n",
    "#     new_df = ak.with_field(new_df, (new_df.cscRechitClusterNRechitChamberPlus13+new_df.cscRechitClusterNRechitChamberMinus13)/new_df.cscRechitClusterSize, \"cscRechitClusterFracR3\")\n",
    "    \n",
    "    #forward hits branch\n",
    "    new_df = ak.with_field(new_df, new_df.cscRechitClusterNRechitChamberPlus11+new_df.cscRechitClusterNRechitChamberMinus11+new_df.cscRechitClusterNRechitChamberPlus12 + new_df.cscRechitClusterNRechitChamberMinus12, \"forward_hits\")\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "622a4317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022EE\n",
      "MC\n",
      "finished csc branches\n",
      "at muon variables\n",
      "now data\n",
      "finished csc branches\n",
      "at muon variables\n"
     ]
    }
   ],
   "source": [
    "events_MC_dict = {}; events_data_dict = {}\n",
    "for campaign in list(events_MC_full_dict.keys()):\n",
    "    print(campaign)\n",
    "    print(\"MC\")\n",
    "    events_MC = getClusterBranches(events_MC_full_dict[campaign], campaign, True)\n",
    "    print(\"now data\")\n",
    "    events_data = getClusterBranches(events_data_full_dict[campaign], campaign, False)\n",
    "    events_MC_dict[campaign] = events_MC\n",
    "    events_data_dict[campaign] = events_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00150cd9",
   "metadata": {},
   "source": [
    "### Code to Mask Data According to Specific Cuts - Low MET and High MET, along with cutflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f2a46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeForwardVetoMask(events, mask, forwardVetoList: list=[]):\n",
    "    forwardMask = mask\n",
    "    if \"forward_veto\" in forwardVetoList:\n",
    "        forwardMask = ak.mask(forwardMask, events.forward_hits==0)\n",
    "    if \"forward_veto_mod\" in forwardVetoList:\n",
    "        forwardMask = ak.mask(forwardMask, events.forward_max_chamber==False)\n",
    "    if \"forward_veto_highMET\" in forwardVetoList:\n",
    "        forwardMask = ak.mask(forwardMask, (events.cscRechitClusterNRechitChamberPlus11+events.cscRechitClusterNRechitChamberMinus11)/events.cscRechitClusterSize<1)\n",
    "    return forwardMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0508581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeEventMask(events, noMaskList: list=[], forwardVetoMaskList: list=[], noCuts=False):\n",
    "    mask = events.cscRechitCluster_matchToProbeMuon\n",
    "    mask = makeForwardVetoMask(events, mask, forwardVetoMaskList)\n",
    "    #mask out hotspot automatically\n",
    "    mask=ak.mask(mask, np.logical_or(np.logical_and(np.logical_or(events.cscRechitClusterPhi<-0.3,events.cscRechitClusterPhi>0.6),abs(events.cscRechitClusterPhi)<2.8), events.cscRechitClusterEta>-1.9))\n",
    "    if noCuts:\n",
    "        return mask\n",
    "    if \"timespread_veto\" not in noMaskList:\n",
    "        mask = ak.mask(mask, events.cscRechitClusterTimeSpreadWeightedAll<20)\n",
    "    if \"time_veto\" not in noMaskList:\n",
    "        mask = ak.mask(mask, events.cscRechitClusterTimeWeighted<12.5)\n",
    "        mask = ak.mask(mask, events.cscRechitClusterTimeWeighted>-5)\n",
    "    if \"DNN_veto\" not in noMaskList:\n",
    "        mask = ak.mask(mask, events.cscRechitClusterDNN_bkgMC_plusBeamHalo>0.96)\n",
    "    if \"clusterSize_veto\" not in noMaskList:\n",
    "        mask = ak.mask(mask, events.cscRechitClusterSize>160)\n",
    "    if \"NStation10_veto\" not in noMaskList:\n",
    "        mask = ak.mask(mask, events.cscRechitClusterNStation10>1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4563b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeEventMaskHighMET(events, noMaskList: list=[], forwardVetoMaskList: list=[], noCuts=False):\n",
    "    mask = events.cscRechitCluster_matchToProbeMuon\n",
    "    mask = makeForwardVetoMask(events, mask, forwardVetoMaskList)\n",
    "    #mask out hotspot automatically\n",
    "    mask=ak.mask(mask, np.logical_or(np.logical_and(np.logical_or(events.cscRechitClusterPhi<-0.3,events.cscRechitClusterPhi>0.6),abs(events.cscRechitClusterPhi)<2.8), events.cscRechitClusterEta>-1.9))\n",
    "    if noCuts:\n",
    "        return mask\n",
    "    if \"timespread_veto\" not in noMaskList: #not actually applied in the analysis\n",
    "        mask = ak.mask(mask, events.cscRechitClusterTimeSpreadWeightedAll<20)\n",
    "    if \"time_veto\" not in noMaskList:\n",
    "        mask = ak.mask(mask, events.cscRechitClusterTimeWeighted<12.5)\n",
    "        mask = ak.mask(mask, events.cscRechitClusterTimeWeighted>-5)\n",
    "    if \"DNN_veto\" not in noMaskList: # not actually applied in the analysis\n",
    "        mask = ak.mask(mask, events.cscRechitClusterDNN_bkgMC_plusBeamHalo>0.96)\n",
    "    if \"clusterSize_veto\" not in noMaskList:\n",
    "        mask = ak.mask(mask, events.cscRechitClusterSize>150) #150 instead of 160 for low MET\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3830d516",
   "metadata": {},
   "source": [
    "#### compute efficiencies (no cuts applied other than forward veto or high MET equivalent, except for measurement of forward veto efficiency itself)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "872e605f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################\n",
      "###################################\n",
      "Computing Efficiencies for 2022EE\n",
      "computing low MET efficiencies in Data\n",
      "Data Denominator:  2835731\n",
      "Forward Veto Efficiency:  1.8570167621682028\n",
      "Modified Forward Veto Efficiency:  66.37135186659101\n",
      "clusters passing new forward veto:  1882113\n",
      "Timespread Veto Efficiency:  36.366573101615046\n",
      "Time Veto Efficiency:  89.91070142972288\n",
      "DNN Veto Efficiency:  23.71095678102218\n",
      "ClusterSize Veto Efficiency:  5.093317988877395\n",
      "NStation10 Veto Efficiency:  85.46484722224436\n"
     ]
    }
   ],
   "source": [
    "# data efficiencies\n",
    "\n",
    "for campaign in list(events_data_dict.keys()):\n",
    "    print(\"###################################\")\n",
    "    print(\"###################################\")\n",
    "    print(f\"Computing Efficiencies for {campaign}\")\n",
    "    events_data = events_data_dict[campaign]\n",
    "    print(\"computing low MET efficiencies in Data\")\n",
    "\n",
    "    denom = ak.count_nonzero(makeEventMask(events_data, [], [], True))\n",
    "    print(\"Data Denominator: \", denom)\n",
    "\n",
    "    num_forward = ak.count_nonzero(makeEventMask(events_data, [], [\"forward_veto\"], True))\n",
    "    print(\"Forward Veto Efficiency: \", num_forward/denom*100)\n",
    "\n",
    "    num_forward_mod = ak.count_nonzero(makeEventMask(events_data, [], [\"forward_veto_mod\"], True))\n",
    "    print(\"Modified Forward Veto Efficiency: \", num_forward_mod/denom*100)\n",
    "\n",
    "    print(\"clusters passing new forward veto: \", num_forward_mod)\n",
    "\n",
    "    num_timespread = ak.count_nonzero(makeEventMask(events_data, ['clusterSize_veto','DNN_veto','time_veto', \"NStation10_veto\"],[\"forward_veto_mod\"]))\n",
    "    print(\"Timespread Veto Efficiency: \", num_timespread/num_forward_mod*100)\n",
    "\n",
    "    num_time = ak.count_nonzero(makeEventMask(events_data, ['clusterSize_veto','DNN_veto','timespread_veto', \"NStation10_veto\"],[\"forward_veto_mod\"]))\n",
    "    print(\"Time Veto Efficiency: \", num_time/num_forward_mod*100)\n",
    "\n",
    "    num_DNN = ak.count_nonzero(makeEventMask(events_data, ['timespread_veto','clusterSize_veto','time_veto', \"NStation10_veto\"],[\"forward_veto_mod\"]))\n",
    "    print(\"DNN Veto Efficiency: \", num_DNN/num_forward_mod*100)\n",
    "\n",
    "    num_clusterSize = ak.count_nonzero(makeEventMask(events_data, ['timespread_veto','DNN_veto','time_veto', \"NStation10_veto\"],[\"forward_veto_mod\"]))\n",
    "    print(\"ClusterSize Veto Efficiency: \", num_clusterSize/num_forward_mod*100)\n",
    "\n",
    "    num_NStation10 = ak.count_nonzero(makeEventMask(events_data, ['timespread_veto','DNN_veto','time_veto', 'clusterSize_veto'],[\"forward_veto_mod\"]))\n",
    "    print(\"NStation10 Veto Efficiency: \", num_NStation10/num_forward_mod*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1299e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################\n",
      "###################################\n",
      "Computing Efficiencies for 2022EE\n",
      "computing low MET efficiencies in MC\n",
      "MC Denominator:  1744175.9760305993\n",
      "Forward Veto Efficiency:  3.232207753951318\n",
      "Modified Forward Veto Efficiency:  70.05398887530895\n",
      "clusters passing new forward veto:  1221864.8442142874\n",
      "Timespread Veto Efficiency:  81.90520820348655\n",
      "Time Veto Efficiency:  94.08350316798364\n",
      "DNN Veto Efficiency:  21.908502648559704\n",
      "ClusterSize Veto Efficiency:  6.051809428316391\n",
      "NStation10 Veto Efficiency:  70.97186548816615\n"
     ]
    }
   ],
   "source": [
    "# MC efficiencies\n",
    "\n",
    "for campaign in list(events_MC_dict.keys()):\n",
    "    print(\"###################################\")\n",
    "    print(\"###################################\")\n",
    "    print(f\"Computing Efficiencies for {campaign}\")\n",
    "    events_MC = events_MC_dict[campaign]\n",
    "    print(\"computing low MET efficiencies in MC\")\n",
    "\n",
    "    denom = ak.sum(ak.mask(events_MC.weight_total, makeEventMask(events_MC, [], [], True)))\n",
    "    print(\"MC Denominator: \", denom)\n",
    "\n",
    "    num_forward = ak.sum(ak.mask(events_MC.weight_total, makeEventMask(events_MC, [], [\"forward_veto\"], True)))\n",
    "    print(\"Forward Veto Efficiency: \", num_forward/denom*100)\n",
    "\n",
    "    num_forward_mod = ak.sum(ak.mask(events_MC.weight_total, makeEventMask(events_MC, [], [\"forward_veto_mod\"], True)))\n",
    "    print(\"Modified Forward Veto Efficiency: \", num_forward_mod/denom*100)\n",
    "    \n",
    "    print(\"clusters passing new forward veto: \", num_forward_mod)\n",
    "\n",
    "    num_timespread = ak.sum(ak.mask(events_MC.weight_total, makeEventMask(events_MC, ['clusterSize_veto','DNN_veto','time_veto', \"NStation10_veto\"],[\"forward_veto_mod\"])))\n",
    "    print(\"Timespread Veto Efficiency: \", num_timespread/num_forward_mod*100)\n",
    "\n",
    "    num_time = ak.sum(ak.mask(events_MC.weight_total, makeEventMask(events_MC, ['clusterSize_veto','DNN_veto','timespread_veto', \"NStation10_veto\"],[\"forward_veto_mod\"])))\n",
    "    print(\"Time Veto Efficiency: \", num_time/num_forward_mod*100)\n",
    "\n",
    "    num_DNN = ak.sum(ak.mask(events_MC.weight_total, makeEventMask(events_MC, ['timespread_veto','clusterSize_veto','time_veto', \"NStation10_veto\"],[\"forward_veto_mod\"])))\n",
    "    print(\"DNN Veto Efficiency: \", num_DNN/num_forward_mod*100)\n",
    "\n",
    "    num_clusterSize = ak.sum(ak.mask(events_MC.weight_total, makeEventMask(events_MC, ['timespread_veto','DNN_veto','time_veto', \"NStation10_veto\"],[\"forward_veto_mod\"])))\n",
    "    print(\"ClusterSize Veto Efficiency: \", num_clusterSize/num_forward_mod*100)\n",
    "\n",
    "    num_NStation10 = ak.sum(ak.mask(events_MC.weight_total, makeEventMask(events_MC, ['timespread_veto','DNN_veto','time_veto', 'clusterSize_veto'],[\"forward_veto_mod\"])))\n",
    "    print(\"NStation10 Veto Efficiency: \", num_NStation10/num_forward_mod*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0daf3b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################\n",
      "###################################\n",
      "Computing Efficiencies for 2022EE\n",
      "computing high MET efficiencies in Data\n",
      "Data Denominator:  2835731\n",
      "Forward Veto Efficiency:  99.99583881545887\n",
      "Timespread Veto Efficiency:  56.0893518454721\n",
      "Time Veto Efficiency:  144.19737655851515\n",
      "DNN Veto Efficiency:  43.3902318573572\n",
      "ClusterSize Veto Efficiency:  8.469099565066394\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for campaign in list(events_MC_dict.keys()):\n",
    "    print(\"###################################\")\n",
    "    print(\"###################################\")\n",
    "    print(f\"Computing Efficiencies for {campaign}\")\n",
    "    events_data = events_data_dict[campaign]\n",
    "    \n",
    "    # data efficiencies\n",
    "    print(\"computing high MET efficiencies in Data\")\n",
    "\n",
    "    denom_noForward = ak.count_nonzero(makeEventMaskHighMET(events_data, [], [], True))\n",
    "    print(\"Data Denominator: \", denom_noForward)\n",
    "\n",
    "\n",
    "    num_forward = ak.count_nonzero(makeEventMaskHighMET(events_data, [], [\"forward_veto_highMET\"], True))\n",
    "    print(\"Forward Veto Efficiency: \", num_forward/denom_noForward*100)\n",
    "\n",
    "    num_timespread = ak.count_nonzero(makeEventMaskHighMET(events_data, ['clusterSize_veto','DNN_veto','time_veto']))\n",
    "    print(\"Timespread Veto Efficiency: \", num_timespread/denom*100)\n",
    "\n",
    "    num_time = ak.count_nonzero(makeEventMaskHighMET(events_data, ['clusterSize_veto','DNN_veto','timespread_veto']))\n",
    "    print(\"Time Veto Efficiency: \", num_time/denom*100)\n",
    "\n",
    "    num_DNN = ak.count_nonzero(makeEventMaskHighMET(events_data, ['timespread_veto','clusterSize_veto','time_veto']))\n",
    "    print(\"DNN Veto Efficiency: \", num_DNN/denom*100)\n",
    "\n",
    "\n",
    "    num_clusterSize = ak.count_nonzero(makeEventMaskHighMET(events_data, ['timespread_veto','DNN_veto','time_veto']))\n",
    "    print(\"ClusterSize Veto Efficiency: \", num_clusterSize/denom*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5453897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################\n",
      "###################################\n",
      "Computing Efficiencies for 2022EE\n",
      "computing high MET efficiencies in MC\n",
      "MC Denominator:  1744175.9760305993\n",
      "Forward Veto Efficiency:  100.0\n",
      "Timespread Veto Efficiency:  79.37065781417553\n",
      "Time Veto Efficiency:  92.82720015922428\n",
      "DNN Veto Efficiency:  25.502123578333073\n",
      "ClusterSize Veto Efficiency:  6.478298096229606\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for campaign in list(events_MC_dict.keys()):\n",
    "    print(\"###################################\")\n",
    "    print(\"###################################\")\n",
    "    print(f\"Computing Efficiencies for {campaign}\")\n",
    "    events_MC = events_MC_dict[campaign]\n",
    "\n",
    "    print(\"computing high MET efficiencies in MC\")\n",
    "\n",
    "\n",
    "    denom = ak.sum(ak.mask(events_MC.weight_total, makeEventMaskHighMET(events_MC, [], [], True)))\n",
    "    print(\"MC Denominator: \", denom)\n",
    "\n",
    "\n",
    "    num_forward = ak.sum(ak.mask(events_MC.weight_total, makeEventMaskHighMET(events_MC,[],[\"forward_veto_highMET\"], True)))\n",
    "    print(\"Forward Veto Efficiency: \", num_forward/denom*100)\n",
    "\n",
    "    num_timespread = ak.sum(ak.mask(events_MC.weight_total, makeEventMaskHighMET(events_MC,['clusterSize_veto','DNN_veto','time_veto'])))\n",
    "    print(\"Timespread Veto Efficiency: \", num_timespread/denom*100)\n",
    "\n",
    "    num_time = ak.sum(ak.mask(events_MC.weight_total, makeEventMaskHighMET(events_MC,['timespread_veto','clusterSize_veto','DNN_veto'])))\n",
    "    print(\"Time Veto Efficiency: \", num_time/denom*100)\n",
    "\n",
    "    num_DNN = ak.sum(ak.mask(events_MC.weight_total, makeEventMaskHighMET(events_MC,['timespread_veto','clusterSize_veto','time_veto'])))\n",
    "    print(\"DNN Veto Efficiency: \", num_DNN/denom*100)\n",
    "\n",
    "    num_clusterSize = ak.sum(ak.mask(events_MC.weight_total, makeEventMaskHighMET(events_MC, ['timespread_veto','DNN_veto','time_veto'])))\n",
    "    print(\"ClusterSize Veto Efficiency: \", num_clusterSize/denom*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99b4759",
   "metadata": {},
   "source": [
    "### Helper functions to make histograms and style them appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ccc405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt.gStyle.SetOptStat(0)\n",
    "def make_ratio_plot(h_list_in, title = \"\", label = \"\", fit = False, in_tags = None, ratio_bounds = [0.1, 4], logy = False, ratio_index = 0, draw_opt = ['E2','E1'], text = \"\", scale=False, scales = [1,1]):\n",
    "    h_list = []\n",
    "    if in_tags == None:\n",
    "        tag = []\n",
    "    else:\n",
    "        tag = in_tags\n",
    "    for i, h in enumerate(h_list_in):\n",
    "        h_list.append(h.Clone('h{}aux{}'.format(i, label)))\n",
    "        if in_tags == None:\n",
    "            tag.append(h.GetTitle())\n",
    "    #print(\"tags: \", tag)\n",
    "    c_out = rt.TCanvas(\"c_out_ratio\"+label, \"c_out_ratio\"+label, 800, 800)\n",
    "    pad1 = rt.TPad(\"pad1\", \"pad1\", 0, 0.3, 1, 1.0)\n",
    "    pad1.SetBottomMargin(0.03)\n",
    "    pad1.SetLeftMargin(0.15)\n",
    "    pad1.SetRightMargin(0.04)# pad2.SetGrid()\n",
    "    if logy:\n",
    "        pad1.SetLogy()\n",
    "\n",
    "    pad1.Draw()\n",
    "    pad1.cd()\n",
    "\n",
    "    leg = rt.TLegend(0.5, 0.65, 0.9, 0.92)\n",
    "    leg = rt.TLegend(0.7, 0.65, 0.9, 0.92)\n",
    "\n",
    "    #leg = rt.TLegend(0.2, 0.7, 0.5, 0.9)\n",
    "    # leg = rt.TLegend(0.7, 0.2, 0.9, 0.4)\n",
    "    leg.SetBorderSize(0)\n",
    "    leg.SetTextSize(0.045)\n",
    "    leg.SetFillStyle(0)\n",
    "    c_out.cd(1)\n",
    "\n",
    "    scaled_h_list = []\n",
    "    if scale:\n",
    "        for i, h_unscaled in enumerate(h_list):\n",
    "            #h = h_unscaled.Clone()\n",
    "            #h = h_unscaled.Scale(1/scales[i])\n",
    "            #scaled_h_list.append(h_unscaled.Clone())\n",
    "            h_unscaled.Scale(1/scales[i])\n",
    "            scaled_h_list.append(h_unscaled)\n",
    "    else:\n",
    "        #for i, h_unscaled in enumerate(h_list):\n",
    "            #h = h_unscaled.Clone()\n",
    "            #scaled_h_list.append(h)\n",
    "        scaled_h_list = h_list\n",
    "    for i, h in enumerate(scaled_h_list):\n",
    "        h.GetXaxis().SetLabelSize(0)\n",
    "        h.GetXaxis().SetTitle(label)\n",
    "        h.GetYaxis().SetRangeUser(0, 1.1*max(map(lambda x: x.GetMaximum(), scaled_h_list)))\n",
    "        if logy and not scale:\n",
    "            h.GetYaxis().SetRangeUser(10e-2, 2*max(map(lambda x: x.GetMaximum(), scaled_h_list)))\n",
    "        if logy and scale:\n",
    "            h.GetYaxis().SetRangeUser(10e-4, 1)\n",
    "        h.GetYaxis().SetTitleOffset(1.0)\n",
    "        h.GetYaxis().SetTitleSize(0.06)\n",
    "        h.GetYaxis().SetLabelSize(0.05)\n",
    "        \n",
    "        if scale:\n",
    "            y_title = \"Fraction of Events\"\n",
    "        else:\n",
    "            y_title = \"Events\"\n",
    "        \n",
    "        h.GetYaxis().SetTitle()\n",
    "        h.SetTitle(f\"{title};adsf;{y_title}\")\n",
    "        #if ratio_index == 0:h.DrawCopy(\"hist\")\n",
    "        '''\n",
    "        h.SetFillColor(h_list_in[i].GetLineColor())\n",
    "        h.SetFillStyle(3002)\n",
    "        #h.SetStats(1)\n",
    "        h.SetLineColor(h_list_in[i].GetLineColor())\n",
    "        h.SetLineWidth(2)\n",
    "        h.SetMarkerColor(h_list_in[i].GetLineColor())\n",
    "        h.SetMarkerSize(2)\n",
    "        # if ratio_index == 0:\n",
    "        #     # h.DrawCopy(\"hist\")\n",
    "        #     h.DrawCopy(draw_opt[i]+'same')\n",
    "        # else:h.DrawCopy(draw_opt[i])\n",
    "        #if ratio_index == 0 :h.DrawCopy(draw_opt[i]+\"same\")\n",
    "        #h.DrawCopy(\"E2 HIST\")\n",
    "        '''\n",
    "        if i==0:\n",
    "            h.SetLineWidth(4)\n",
    "            h.DrawCopy(\"hist\")\n",
    "            #h.SetFillStyle(0)\n",
    "            h.SetFillColor(h_list_in[i].GetLineColor())\n",
    "            h.SetFillStyle(3002)\n",
    "            #h.SetStats(1)\n",
    "            h.SetLineColor(h_list_in[i].GetLineColor())\n",
    "            h.SetLineWidth(2)\n",
    "            h.SetMarkerColor(h_list_in[i].GetLineColor())\n",
    "            h.SetMarkerSize(2)\n",
    "            h.DrawCopy(draw_opt[i] + \"same\")\n",
    "            #h.SetFillStyle(0)\n",
    "        else:\n",
    "            h.SetLineWidth(2)\n",
    "            h.DrawCopy(draw_opt[i] + \"same\")\n",
    "        #else:h.DrawCopy(draw_opt[i])\n",
    "        if len(text)>0:\n",
    "            l = rt.TLatex()\n",
    "            l.SetTextSize(0.045)\n",
    "            if logy:l.DrawLatex((h.GetXaxis().GetXmax()-h.GetXaxis().GetXmin())*0.1+h.GetXaxis().GetXmin() , h.GetMaximum()/10, text)\n",
    "            else:l.DrawLatex((h.GetXaxis().GetXmax()-h.GetXaxis().GetXmin())*0.1+h.GetXaxis().GetXmin() , h.GetMaximum()*0.8, text)\n",
    "        #if i==1:\n",
    "            #h.DrawCopy(draw_opt[i]+\"same\")\n",
    "       #     h.Draw(\"E1 same\")\n",
    "\n",
    "        leg.AddEntry(h, tag[i], \"lep\")\n",
    "    leg.Draw(\"same\")\n",
    "    cmsText = rt.TLatex()\n",
    "\n",
    "    cmsText.SetNDC(True)\n",
    "\n",
    "    cmsText.SetTextFont(42);  \n",
    "    cmsText.SetTextSize(0.045);\n",
    "    cmsText.SetTextAlign(11); \n",
    "    cmsText.DrawLatex(0.17, 0.85, \"#bf{CMS Work in progress}\") \n",
    "\n",
    "    c_out.cd()\n",
    "    pad2 = rt.TPad(\"pad2\", \"pad2\", 0, 0, 1, 0.3)\n",
    "    pad2.SetTopMargin(0.03)\n",
    "    pad2.SetBottomMargin(0.25)\n",
    "    pad2.SetLeftMargin(0.15)\n",
    "    pad2.SetRightMargin(0.04)# pad2.SetGrid()\n",
    "    pad2.Draw()\n",
    "    pad2.cd()\n",
    "    band = scaled_h_list[ratio_index].Clone('h_band')\n",
    "    for j in range(band.GetXaxis().GetNbins()):\n",
    "        band.SetBinContent(j+1, 1.0)\n",
    "        if h_list[ratio_index].GetBinContent(j+1) == 0:\n",
    "            band.SetBinError(j+1, 0.0)\n",
    "        else:\n",
    "            band.SetBinError(j+1, scaled_h_list[ratio_index].GetBinError(j+1)/scaled_h_list[ratio_index].GetBinContent(j+1))\n",
    "            #print(j, h_list_in[0].GetBinError(j+1)/h_list_in[0].GetBinContent(j+1))\n",
    "    band.SetFillColor(scaled_h_list[ratio_index].GetLineColor())\n",
    "\n",
    "    band.SetFillStyle(3002)\n",
    "    band.SetLineColor(scaled_h_list[ratio_index].GetLineColor())\n",
    "    #band.SetFillColorAlpha(0,0)\n",
    "    #band.SetLineColor(0)\n",
    "    \n",
    "    band.GetYaxis().SetTitleOffset(0.5)\n",
    "    band.GetYaxis().SetRangeUser(ratio_bounds[0], ratio_bounds[1])\n",
    "    band.GetYaxis().SetTitleSize(0.11)\n",
    "    band.GetYaxis().SetLabelSize(0.12)\n",
    "    band.GetYaxis().SetNdivisions(506)\n",
    "    band.GetXaxis().SetTitleOffset(0.95)\n",
    "    band.GetXaxis().SetTitleSize(0.12)\n",
    "    band.GetXaxis().SetLabelSize(0.12)\n",
    "    band.GetXaxis().SetTickSize(0.07)\n",
    "    \n",
    "    band.SetYTitle('Ratio with {}'.format(tag[ratio_index]))\n",
    "    band.SetXTitle(label)\n",
    "    band.SetTitle(\"\")\n",
    "    band.DrawCopy('E2')\n",
    "    ln = rt.TLine(h.GetXaxis().GetXmin(), 1, h.GetXaxis().GetXmax(), 1)\n",
    "    ln.SetLineWidth(3)\n",
    "    ln.SetLineColor(scaled_h_list[ratio_index].GetLineColor())\n",
    "    ln.DrawLine(h.GetXaxis().GetXmin(), 1, h.GetXaxis().GetXmax(), 1)\n",
    "     \n",
    "    #print(ratio_index)\n",
    "    for i, h in enumerate(scaled_h_list):\n",
    "        if i == ratio_index:\n",
    "            continue\n",
    "        else:\n",
    "            if fit:h.GetFunction(\"expo\")\n",
    "            h.Divide(scaled_h_list[ratio_index])\n",
    "            # h.GetYaxis().SetTitleOffset(0.6)\n",
    "            # h.GetYaxis().SetRangeUser(ratio_bounds[0], ratio_bounds[1])\n",
    "            # h.GetYaxis().SetTitleSize(0.12)\n",
    "            # h.GetYaxis().SetLabelSize(0.12)\n",
    "            # h.GetYaxis().SetNdivisions(506)\n",
    "            # h.GetXaxis().SetTitleOffset(0.95)\n",
    "            # h.GetXaxis().SetTitleSize(0.12)\n",
    "            # h.GetXaxis().SetLabelSize(0.12)\n",
    "            # h.GetXaxis().SetTickSize(0.07)\n",
    "            # h.SetYTitle('Ratio with {}'.format(tag[0]))\n",
    "            # h.SetTitle(\"\")\n",
    "            #set relative error of ratio to be the relative error of data\n",
    "            for j in range(h.GetXaxis().GetNbins()):\n",
    "                if h_list[i].GetBinContent(j+1) == 0:\n",
    "                    h.SetBinError(j+1, 0.0)\n",
    "                else:\n",
    "                    h.SetBinError(j+1, h_list_in[i].GetBinError(j+1)/h_list_in[i].GetBinContent(j+1)*h.GetBinContent(j+1))\n",
    "            h.Draw('same'+draw_opt[i])\n",
    "    \n",
    "    pad2.Update()\n",
    "    \n",
    "    c_out.pad1 = pad1\n",
    "    c_out.pad2 = pad2\n",
    "    c_out.h_list = h_list\n",
    "    c_out.leg = leg\n",
    "    \n",
    "    \n",
    "    return c_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3a22776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to build histograms\n",
    "\n",
    "def makeHists(events_data, events_MC, branch, mask_array, bins_tuple, highMET = False):\n",
    "    print(f\"on branch {branch}\")\n",
    "    \n",
    "    nbins, lowBin, highBin = bins_tuple\n",
    "    \n",
    "    #loop over three types of plots (no cuts, just forward veto, all cuts other than that measured)\n",
    "    name_strs = [\"noCuts\", \"forwardVeto\", \"modifiedForwardVeto\", \"allOtherCuts\"]\n",
    "    masks = [[], [], [], mask_array]\n",
    "    masks_forwardVeto = [[],[\"forward_veto\"],[\"forward_veto_mod\"],[\"forward_veto_mod\"]]\n",
    "    masks_forwardVeto_highMET = [[],[\"forward_veto_highMET\"],[], []]\n",
    "    mask_bools = [True, True, True, False]\n",
    "    \n",
    "    hist_info = {}\n",
    "    for plotType, mask_list, mask_list_forwardVeto, mask_list_forwardVeto_highMET, mask_bool in zip(name_strs, masks, masks_forwardVeto, masks_forwardVeto_highMET, mask_bools):\n",
    "        #compute relevant mask for particular plot\n",
    "        if plotType==\"modifiedForwardVeto\" and highMET:continue\n",
    "        if not highMET:\n",
    "            mask_data = makeEventMask(events_data, mask_list, mask_list_forwardVeto, mask_bool)\n",
    "            mask_MC = makeEventMask(events_MC, mask_list, mask_list_forwardVeto, mask_bool)\n",
    "        else:\n",
    "            mask_data = makeEventMaskHighMET(events_data, mask_list, mask_list_forwardVeto_highMET, mask_bool)\n",
    "            mask_MC = makeEventMaskHighMET(events_MC, mask_list, mask_list_forwardVeto_highMET, mask_bool)\n",
    "    \n",
    "        data_tree = events_data[mask_data]\n",
    "        data_tree = data_tree[~ak.is_none(data_tree)]\n",
    "        \n",
    "        MC_tree = events_MC[mask_MC]\n",
    "        MC_tree = MC_tree[~ak.is_none(MC_tree)]\n",
    "    \n",
    "        #initialize data and MC histograms\n",
    "        data = rt.TH1F(\"Data\", \"Data\", nbinsx=nbins, xlow = lowBin, xup=highBin)\n",
    "        MC = rt.TH1F(\"MC\", \"MC\", nbinsx=nbins, xlow = lowBin, xup=highBin)\n",
    "\n",
    "\n",
    "        #build data hist\n",
    "        data_arr = np.array(data_tree[branch], dtype=np.float64)\n",
    "        data_size = np.size(data_arr)\n",
    "        data_weights = np.ones(data_size, dtype=np.float64)\n",
    "        data.FillN(data_size, data_arr, data_weights)\n",
    "        data.SetLineColor(rt.kBlack)\n",
    "        data.SetFillStyle(0)\n",
    "        \n",
    "        #build MC hist\n",
    "        MC_arr = np.array(MC_tree[branch], dtype=np.float64)\n",
    "        MC_size = np.size(MC_arr)\n",
    "        MC_weights = np.array(MC_tree[\"weight_total\"])\n",
    "        MC.FillN(MC_size, MC_arr, MC_weights)\n",
    "        MC.SetLineColor(rt.kRed)\n",
    "        MC.SetFillStyle(0)\n",
    "        \n",
    "        sumOfWeights = ak.sum(MC_tree[\"weight_total\"])\n",
    "        \n",
    "        hist_info[branch+\"_\"+plotType] = {\"MC_hist\": MC, \"data_hist\": data, \n",
    "                                          \"MC_weights\": sumOfWeights, \"data_weights\": data_size}\n",
    "        \n",
    "    return hist_info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "910482e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dictionary with relevant plot info\n",
    "plot_info = {\n",
    "#             \"ZMass\": {\"filename_base\":\"ZMass\", \"title\":\"Dimuon Mass Distribution\", \n",
    "#                        \"xlabel\":\"Dimuon Mass [GeV]\", \"masks\":[],\"bins\":(80, 0, 150), \"logy\":False},\n",
    "#             \"puppiMet\": {\"filename_base\":\"puppiMet\", \"title\":\"PUPPI MET Distribution\", \n",
    "#                        \"xlabel\":\"PUPPI MET [GeV]\", \"masks\":[],\"bins\":(60, -5, 100), \"logy\":False},\n",
    "#             \"puppiMetPhi\": {\"filename_base\":\"puppiMetPhi\", \"title\":\"PUPPI MET Phi Distribution\", \n",
    "#                        \"xlabel\":\"PUPPI MET Phi\", \"masks\":[],\"bins\":(30, -4, 4), \"logy\":False},\n",
    "#             \"cscRechitClusterMuonDeltaR\": {\"filename_base\":\"cluster_muon_deltaR\", \"title\":\"deltaR(cluster, muon)\", \n",
    "#                        \"xlabel\":\"deltaR(cluster, muon)\", \"masks\":[],\"bins\":(50, 0, 0.5), \"logy\":False},\n",
    "            \n",
    "#             \"probe_pT\": {\"filename_base\":\"probe_pT\", \"title\":\"pT of Muon Matched to Cluster\", \n",
    "#                        \"xlabel\":\"pT [GeV]\", \"masks\":[],\"bins\":(25, 0, 100), \"logy\":False},\n",
    "#             \"tag_pT\": {\"filename_base\":\"tag_pT\", \"title\":\"pT of Muon Not Matched to Cluster\", \n",
    "#                        \"xlabel\":\"pT [GeV]\", \"masks\":[],\"bins\":(25, 0, 100), \"logy\":False},\n",
    "#             \"probe_phi\": {\"filename_base\":\"probe_phi\", \"title\":\"Phi of Muon Matched to Cluster\", \n",
    "#                        \"xlabel\":\"pT [GeV]\", \"masks\":[],\"bins\":(60, -4, 4), \"logy\":False},\n",
    "#             \"tag_phi\": {\"filename_base\":\"tag_phi\", \"title\":\"Phi of Muon Not Matched to Cluster\", \n",
    "#                        \"xlabel\":\"pT [GeV]\", \"masks\":[],\"bins\":(60, -4, 4), \"logy\":False},\n",
    "#             \"probe_eta\": {\"filename_base\":\"probe_eta\", \"title\":\"Eta of Muon Matched to Cluster\", \n",
    "#                        \"xlabel\":\"pT [GeV]\", \"masks\":[],\"bins\":(60, -4, 4), \"logy\":False},\n",
    "#             \"tag_eta\": {\"filename_base\":\"tag_eta\", \"title\":\"Eta of Muon Not Matched to Cluster\", \n",
    "#                        \"xlabel\":\"pT [GeV]\", \"masks\":[],\"bins\":(60, -4, 4), \"logy\":False},\n",
    "            \n",
    "            \"cscRechitClusterSize\": {\"filename_base\":\"cscRechitClusterSize\", \"title\":\"Cluster Size Distribution\", \n",
    "                       \"xlabel\":\"N_{hits}\", \"masks\":[\"clusterSize_veto\"],\"bins\":(40, 0, 400), \"logy\":True},\n",
    "            \"cscRechitClusterDNN_bkgMC_plusBeamHalo\": {\"filename_base\":\"DNN_Score\", \"title\":\"DNN Score Distribution\", \n",
    "                       \"xlabel\":\"DNN Score\", \"masks\":[\"DNN_veto\"],\"bins\":(12, 0.52, 1), \"logy\":True},\n",
    "            \"cscRechitClusterTimeWeighted\": {\"filename_base\":\"Cluster_Time\", \"title\":\"Weighted Cluster Time Distribution\", \n",
    "                       \"xlabel\":\"Weighted Cluster Time [ns]\", \"masks\":[\"time_veto\"],\"bins\":(60, -8, 20), \"logy\":False},\n",
    "            \"cscRechitClusterTimeSpreadWeightedAll\": {\"filename_base\":\"Cluster_Timespread\", \"title\":\"Weighted Cluster Timespread Distribution\", \n",
    "                       \"xlabel\":\"Weighted Cluster Timespread [ns]\", \"masks\":[\"timespread_veto\"],\"bins\":(60, 0, 50), \"logy\":False},\n",
    "            \"cscRechitClusterEta\": {\"filename_base\":\"cluster_eta\", \"title\":\"Cluster Eta\", \n",
    "                       \"xlabel\":\"Eta\", \"masks\":[],\"bins\":(60, -4, 4), \"logy\":False},\n",
    "            \"cscRechitClusterPhi\": {\"filename_base\":\"cluster_phi\", \"title\":\"Cluster Phi\", \n",
    "                       \"xlabel\":\"Phi\", \"masks\":[],\"bins\":(60, -4, 4), \"logy\":False},\n",
    "            \"cscRechitClusterNStation10\": {\"filename_base\":\"cluster_NStation10\", \"title\":\"Number of Stations with >=10 Rechits\", \n",
    "                       \"xlabel\":\"# of Stations\", \"masks\":[],\"bins\":(5, 0, 5), \"logy\":False}\n",
    "            \n",
    "#             \"cscRechitClusterXSpread\": {\"filename_base\":\"cscRechitClusterXSpread\", \"title\":\"Cluster X Spread\", \n",
    "#                        \"xlabel\":\"X Spread [cm]\", \"masks\":[],\"bins\":(25, -5, 150), \"logy\": False},\n",
    "#             \"cscRechitClusterYSpread\": {\"filename_base\":\"cscRechitClusterYSpread\", \"title\":\"Cluster Y Spread\", \n",
    "#                        \"xlabel\":\"Y Spread [cm]\", \"masks\":[],\"bins\":(25, -5, 150), \"logy\": False},\n",
    "#             \"cscRechitClusterZSpread\": {\"filename_base\":\"cscRechitClusterZSpread\", \"title\":\"Cluster Z Spread\", \n",
    "#                        \"xlabel\":\"Z Spread [cm]\", \"masks\":[],\"bins\":(25, -5, 200), \"logy\": False},\n",
    "#             \"cscRechitClusterXYSpread\": {\"filename_base\":\"cscRechitClusterXYSpread\", \"title\":\"Cluster XY Spread\", \n",
    "#                        \"xlabel\":\"XY Spread [cm]\", \"masks\":[],\"bins\":(25, -5, 150), \"logy\": False},\n",
    "#             \"cscRechitClusterRSpread\": {\"filename_base\":\"cscRechitClusterRSpread\", \"title\":\"Cluster R Spread\", \n",
    "#                        \"xlabel\":\"R Spread [cm]\", \"masks\":[],\"bins\":(25, -5, 150), \"logy\": False},\n",
    "#             \"cscRechitClusterSkewX\": {\"filename_base\":\"cscRechitClusterSkewX\", \"title\":\"Cluster X Skew\", \n",
    "#                        \"xlabel\":\"X Skew [cm]\", \"masks\":[],\"bins\":(25, -150, 150), \"logy\": False},\n",
    "#             \"cscRechitClusterSkewY\": {\"filename_base\":\"cscRechitClusterSkewY\", \"title\":\"Cluster Y Skew\", \n",
    "#                        \"xlabel\":\"Y Skew [cm]\", \"masks\":[],\"bins\":(25, -150, 150), \"logy\": False},\n",
    "#              \"cscRechitClusterSkewZ\": {\"filename_base\":\"cscRechitClusterSkewZ\", \"title\":\"Cluster Z Skew\", \n",
    "#                        \"xlabel\":\"Z Skew [cm]\", \"masks\":[],\"bins\":(25, -150, 150), \"logy\": False},\n",
    "#             \"cscRechitClusterSkewX\": {\"filename_base\":\"cscRechitClusterSkewX\", \"title\":\"Cluster X Spread\", \n",
    "#                        \"xlabel\":\"X Skew [cm]\", \"masks\":[],\"bins\":(25, -150, 150), \"logy\": False},\n",
    "#             \"cscRechitClusterFracS1\": {\"filename_base\":\"cscRechitClusterFracS1\", \"title\":\"Fraction of Hits in Station 1\", \n",
    "#                        \"xlabel\":\"Station 1 Hits/Total Hits\", \"masks\":[],\"bins\":(25, 0, 1.1), \"logy\": False},\n",
    "#             \"cscRechitClusterFracS2\": {\"filename_base\":\"cscRechitClusterFracS2\", \"title\":\"Fraction of Hits in Station 2\", \n",
    "#                        \"xlabel\":\"Station 2 Hits/Total Hits\", \"masks\":[],\"bins\":(25, 0, 1.1), \"logy\": False},\n",
    "#             \"cscRechitClusterFracS3\": {\"filename_base\":\"cscRechitClusterFracS3\", \"title\":\"Fraction of Hits in Station 3\", \n",
    "#                        \"xlabel\":\"Station 3 Hits/Total Hits\", \"masks\":[],\"bins\":(25, 0, 1.1), \"logy\": False},\n",
    "#             \"cscRechitClusterFracS4\": {\"filename_base\":\"cscRechitClusterFracS4\", \"title\":\"Fraction of Hits in Station 4\", \n",
    "#                        \"xlabel\":\"Station 4 Hits/Total Hits\", \"masks\":[],\"bins\":(25, 0, 1.1), \"logy\": False},\n",
    "#             \"cscRechitClusterFracR1\": {\"filename_base\":\"cscRechitClusterFracR1\", \"title\":\"Fraction of Hits in Ring 1\", \n",
    "#                        \"xlabel\":\"Ring 1 Hits/Total Hits\", \"masks\":[],\"bins\":(25, 0, 1.1), \"logy\": False},\n",
    "#             \"cscRechitClusterFracR2\": {\"filename_base\":\"cscRechitClusterFracR2\", \"title\":\"Fraction of Hits in Ring 2\", \n",
    "#                        \"xlabel\":\"Ring 2 Hits/Total Hits\", \"masks\":[],\"bins\":(25, 0, 1.1), \"logy\": False},\n",
    "#             \"cscRechitClusterFracR3\": {\"filename_base\":\"cscRechitClusterFracR3\", \"title\":\"Fraction of Hits in Ring 3\", \n",
    "#                        \"xlabel\":\"Ring 3 Hits/Total Hits\", \"masks\":[],\"bins\":(25, 0, 1.1), \"logy\": False}\n",
    "            \n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed064de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022EE\n",
      "on branch cscRechitClusterSize\n"
     ]
    }
   ],
   "source": [
    "full_individual_plot_info = {}\n",
    "for campaign in list(events_MC_dict.keys()):\n",
    "    print(campaign)\n",
    "    events_MC = events_MC_dict[campaign]\n",
    "    events_data = events_data_dict[campaign]\n",
    "    individual_plot_info = {}\n",
    "    for branch, info_dict in plot_info.items():\n",
    "        #if branch!=\"cscRechitClusterSize\" and branch!=\"cscRechitClusterDNN_bkgMC_plusBeamHalo\":continue\n",
    "        filename_base = info_dict[\"filename_base\"]\n",
    "        hist_dict = makeHists(events_data, events_MC, branch, info_dict[\"masks\"], info_dict[\"bins\"])\n",
    "        for plot_hists, plot_hist_dict in hist_dict.items():\n",
    "            individual_plot_info[plot_hists] = {\"MC_hist\": plot_hist_dict[\"MC_hist\"], \"data_hist\": plot_hist_dict[\"data_hist\"], \n",
    "                        \"file_name\": plot_hists, \"title\": info_dict[\"title\"], \"label\": info_dict[\"xlabel\"], \n",
    "                        \"scales\": [plot_hist_dict[\"MC_weights\"], plot_hist_dict[\"data_weights\"]], \"logy\": info_dict[\"logy\"]}\n",
    "    full_individual_plot_info[campaign] = individual_plot_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded26041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_individual_plot_info_highMET = {}\n",
    "# for campaign in list(events_MC_dict.keys()):\n",
    "#     print(campaign)\n",
    "#     events_MC = events_MC_dict[campaign]\n",
    "#     events_data = events_data_dict[campaign]\n",
    "#     individual_plot_info_highMET = {}\n",
    "#     for branch, info_dict in plot_info.items():\n",
    "#         filename_base = info_dict[\"filename_base\"]\n",
    "#         hist_dict = makeHists(events_data, events_MC, branch, info_dict[\"masks\"], info_dict[\"bins\"], True)\n",
    "#         for plot_hists, plot_hist_dict in hist_dict.items():\n",
    "#             individual_plot_info_highMET[plot_hists] = {\"MC_hist\": plot_hist_dict[\"MC_hist\"], \"data_hist\": plot_hist_dict[\"data_hist\"], \n",
    "#                         \"file_name\": plot_hists, \"title\": info_dict[\"title\"], \"label\": info_dict[\"xlabel\"], \n",
    "#                         \"scales\": [plot_hist_dict[\"MC_weights\"], plot_hist_dict[\"data_weights\"]], \"logy\": info_dict[\"logy\"]}\n",
    "#     full_individual_plot_info_highMET[campaign] = individual_plot_info_highMET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1497329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for campaign, individual_plot_info in full_individual_plot_info.items():\n",
    "    print(campaign)\n",
    "    plot_output = f\"{campaign}_Data_MC_Comp_finalzedSelections\"\n",
    "    os.makedirs(plot_output, exist_ok=True)\n",
    "\n",
    "    for plot_type, plot_info_dict in individual_plot_info.items():\n",
    "        print(plot_type)\n",
    "        for boolScale in [True, False]:\n",
    "            c = make_ratio_plot([plot_info_dict[\"MC_hist\"], plot_info_dict[\"data_hist\"]], title = plot_info_dict[\"title\"], label = plot_info_dict[\"label\"], fit = False, in_tags = None, ratio_bounds = [0.1, 4], logy = plot_info_dict[\"logy\"], ratio_index = 0, draw_opt = ['E2','E1'], text = \"\", scale=boolScale, scales = plot_info_dict[\"scales\"])\n",
    "            if boolScale:\n",
    "                scaleString = \"_normalized\"\n",
    "            else:\n",
    "                scaleString=\"\"\n",
    "            os.makedirs(plot_output+\"/\"+plot_info_dict[\"file_name\"], exist_ok=True)\n",
    "            c.SaveAs(plot_output+\"/\"+plot_info_dict[\"file_name\"]+\"/\"+plot_info_dict[\"file_name\"]+scaleString+\".png\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3448c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for campaign, individual_plot_info_highMET in full_individual_plot_info_highMET.items():\n",
    "#     print(campaign)\n",
    "\n",
    "#     plot_output = f\"{campaign}_preBPix_Data_MC_Comp_finalzedSelections_highMET\"\n",
    "#     os.makedirs(plot_output, exist_ok=True)\n",
    "\n",
    "#     for plot_type, plot_info_dict in individual_plot_info_highMET.items():\n",
    "#         print(plot_type)\n",
    "#         for boolScale in [True, False]:\n",
    "#             c = make_ratio_plot([plot_info_dict[\"MC_hist\"], plot_info_dict[\"data_hist\"]], title = plot_info_dict[\"title\"], label = plot_info_dict[\"label\"], fit = False, in_tags = None, ratio_bounds = [0.1, 4], logy = plot_info_dict[\"logy\"], ratio_index = 0, draw_opt = ['E2','E1'], text = \"\", scale=boolScale, scales = plot_info_dict[\"scales\"])\n",
    "#             if boolScale:\n",
    "#                 scaleString = \"_normalized\"\n",
    "#             else:\n",
    "#                 scaleString=\"\"\n",
    "#             os.makedirs(plot_output+\"/\"+plot_info_dict[\"file_name\"], exist_ok=True)\n",
    "#             c.SaveAs(plot_output+\"/\"+plot_info_dict[\"file_name\"]+\"/\"+plot_info_dict[\"file_name\"]+scaleString+\".png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4647ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_individual_plot_info[\"2023BPix\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034958d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
